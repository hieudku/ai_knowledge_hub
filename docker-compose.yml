version: "3.9"

services:
  openwebui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: openwebui
    restart: unless-stopped
    ports:
      - "3000:8080" # Access at http://localhost:3000
    env_file:
      - .env

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    volumes:
      # ðŸ”¹ Persist local Ollama models
      - ollama:/root/.ollama

      # ðŸ”¹ Persist WebUI data (knowledge, settings, embeddings)
      - openwebui_data:/app/backend/data

      # ðŸ”¹ Optional: Mount your local knowledge directory
      - C:/Users/hieuc/ai_knowledge_hub/outputs:/app/backend/data/knowledge

volumes:
  - ollama:/root/.ollama
  - openwebui_data:/app/backend/data
  - C:/Users/hieuc/ai_knowledge_hub/outputs:/app/backend/data/uploads/mounted-scrapping